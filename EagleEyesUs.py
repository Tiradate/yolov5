# Form implementation generated from reading ui file 'pyinstaller --onefile --windowed main.pycheck.ui'
#
# Created by: PyQt6 UI code generator 6.3.1
#
# WARNING: Any manual changes made to this file will be lost when pyuic6 is
# run again.  Do not edit this file unless you know what you are doing.


from argparse import Namespace
from ast import Break, Str
from ctypes import alignment
from datetime import *
import json
import os, csv
from os.path import exists
import shutil
import re
import sys
from asyncio.windows_events import NULL
from asyncore import read
from pathlib import Path, PureWindowsPath
from traceback import print_stack
from turtle import delay
import time

import cv2
import numpy as np
from pyparsing import null_debug_action
import pytesseract as pyt
from pytesseract import Output
import torch
import torch.backends.cudnn as cudnn
#pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
from PIL import Image

from PyQt6 import QtCore, QtGui, QtWidgets
from PyQt6.QtCore import Qt, QThread, pyqtSignal, pyqtSlot, QDate, QRectF
from PyQt6.QtGui import QIcon, QPixmap, QPainter, QPen #, QPdfWriter
from PyQt6.QtWidgets import QFileDialog, QMessageBox, QTableWidgetItem, QCalendarWidget

#Gen pdf
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib.styles import ParagraphStyle
from reportlab.platypus import Paragraph

#from scipy.ndimage import interpolation as inter
from typing_extensions import Self

#from tcping import Ping
import socket

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

root_p = Path.cwd()
pdfmetrics.registerFont(TTFont('THSarabunNew', str(root_p / 'Font/THSarabunNew.ttf')))

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS
from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, smart_inference_mode

"""class LoadImages:
    # YOLOv5 image/video dataloader, i.e. `python detect.py --source image.jpg/vid.mp4`
    def __init__(self, path, img_size=640, stride=32, auto=True, transforms=None, vid_stride=1):
        files = path
        for p in sorted(path) if isinstance(path, (list, tuple)) else [path]:
            p = str(Path(p).resolve())
            if '*' in p:
                files.extend(sorted(glob.glob(p, recursive=True)))  # glob
            elif os.path.isdir(p):
                files.extend(sorted(glob.glob(os.path.join(p, '*.*'))))  # dir
            elif os.path.isfile(p):
                files.append(p)  # files
            else:
                raise FileNotFoundError(f'{p} does not exist')

        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]
        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]
        ni, nv = len(images), len(videos)

        self.img_size = img_size
        self.stride = stride
        self.files = images + videos
        self.nf = ni + nv  # number of files
        self.video_flag = [False] * ni + [True] * nv
        self.mode = 'image'
        self.auto = auto
        self.transforms = transforms  # optional
        self.vid_stride = vid_stride  # video frame-rate stride
        if any(videos):
            self._new_video(videos[0])  # new video
        else:
            self.cap = None
        assert self.nf > 0, f'No images or videos found in {p}. ' \
                            f'Supported formats are:\nimages: {IMG_FORMATS}\nvideos: {VID_FORMATS}'

    def __iter__(self):
        self.count = 0
        return self

    def __next__(self):
        if self.count == self.nf:
            raise StopIteration
        path = self.files[self.count]

        if self.video_flag[self.count]:
            # Read video
            self.mode = 'video'
            for _ in range(self.vid_stride):
                self.cap.grab()
            ret_val, im0 = self.cap.retrieve()
            while not ret_val:
                self.count += 1
                self.cap.release()
                if self.count == self.nf:  # last video
                    raise StopIteration
                path = self.files[self.count]
                self._new_video(path)
                ret_val, im0 = self.cap.read()

            self.frame += 1
            # im0 = self._cv2_rotate(im0)  # for use if cv2 autorotation is False
            s = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: '

        else:
            # Read image
            self.count += 1
            im0 = cv2.imread(path)  # BGR
            assert im0 is not None, f'Image Not Found {path}'
            s = f'image {self.count}/{self.nf} {path}: '

        if self.transforms:
            im = self.transforms(im0)  # transforms
        else:
            im = letterbox(im0, self.img_size, stride=self.stride, auto=self.auto)[0]  # padded resize
            im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
            im = np.ascontiguousarray(im)  # contiguous

        return path, im, im0, self.cap, s

    def _new_video(self, path):
        # Create a new video capture object
        self.frame = 0
        self.cap = cv2.VideoCapture(path)
        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)
        self.orientation = int(self.cap.get(cv2.CAP_PROP_ORIENTATION_META))  # rotation degrees
        # self.cap.set(cv2.CAP_PROP_ORIENTATION_AUTO, 0)  # disable https://github.com/ultralytics/yolov5/issues/8493

    def _cv2_rotate(self, im):
        # Rotate a cv2 video manually
        if self.orientation == 0:
            return cv2.rotate(im, cv2.ROTATE_90_CLOCKWISE)
        elif self.orientation == 180:
            return cv2.rotate(im, cv2.ROTATE_90_COUNTERCLOCKWISE)
        elif self.orientation == 90:
            return cv2.rotate(im, cv2.ROTATE_180)
        return im

    def __len__(self):
        return self.nf  # number of files"""

class Main():
    source1 = []
    source2 = []
    """def run_predic1(self,
        weights= str(root_p / 'weights/TKKbest.pt'),  # model.pt path(s)
        data= str(root_p / 'data/TKKdata2.yaml'),  # dataset.yaml path
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.5,  # confidence threshold
        iou_thres=0.5,  # NMS IOU threshold
        max_det=3,  # maximum detections per image
        device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        view_img=False,  # show results
        save_crops=True,  # save cropped prediction boxes
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        visualize=False,  # visualize features
        exist_ok=True,  # existing project/name ok, do not increment
        line_thickness=1,  # bounding box thickness (pixels)
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
        vid_stride = 4,
    ):
        while True:
            try:
                self.source1 = check_file(self.source1)  # download

                # Directories
                project = self.dest+'/Entrance'
                
                # Load model
                device = select_device(device)
                model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
                stride, names, pt = model.stride, model.names, model.pt
                imgsz = check_img_size(imgsz, s=stride)  # check image size

                dataset = LoadImages(self.source1, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
                bs = len(dataset)  # batch_size

                # Run inference
                model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup
                seen, windows, dt = 0, [], (Profile(), Profile(), Profile())
                
                for path, im, im0s, vid_cap, s in dataset:
                    
                    im = torch.from_numpy(im).to(device)
                    im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
                    im /= 255  # 0 - 255 to 0.0 - 1.0
                    if len(im.shape) == 3:
                        im = im[None]  # expand for batch dim

                    # Inference
                    visualize = increment_path(self.save_dir1 / Path(path).stem, mkdir=True) if visualize else False
                    pred = model(im, augment=augment, visualize=visualize)

                    # NMS
                    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

                    # Second-stage classifier (optional)
                    # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)

                    # Process predictions
                    for i, det in enumerate(pred):  # per image

                        seen += 1
                        if webcam:  # batch_size >= 1
                            p, im0, frame = path[i], im0s[i].copy(), dataset.count
                            s += f'{i}: '
                        else:
                            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)
                        
                        # Save non model
                        trMill = int(time.time() * 1000)
                        if cam1_contL1 and (trMill - tlMill > self.entr1):
                            self.cam1_snaps_con1 = (self.save_dir1 / cam1_file_name / str(cam1_file_name+'_snap1')).with_suffix('.jpg')
                            cv2.imwrite(self.cam1_snaps_con1, im0)
                            tlMill1 = int(time.time() * 1000)
                            cam1_contL1 = False
                            cam1_contL2 = True
                            VideoThread_3.cam1_snaps_con3 = str(self.save_dir1 / cam1_file_name / cam1_file_name)

                        if cam1_contL2 and (trMill - tlMill1 > self.entr2):
                            self.cam1_snaps_con2 = (self.save_dir1 / cam1_file_name / str(cam1_file_name+'_snap2')).with_suffix('.jpg')
                            cv2.imwrite(self.cam1_snaps_con2, im0)
                            cam1_contL2 = False
                            cam1_contL3 = True

                        p = Path(p)  # to Path
                        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
                        imc = im0.copy() if save_crops else im0  # for save_crop
                        annotator_plate = Annotator(im0, line_width=line_thickness, example="")
                        
                        if len(det):
                            # Rescale boxes from img_size to im0 size
                            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()

                            # Write results
                            for *xyxy, conf, cls in reversed(det):

                                # Convert xyxy to xywh
                                xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                                annotator_plate.box_label(xyxy, None, color=(255,0,0))
                                ready_snap = False
                                while ( 0.6 < round(xywh[0],5) < 0.75 and ready_snap == False) : #and ((0.02*0.03) < (xywh[2]*xywh[3]) < (0.1*0.1))
                                    
                                    if ((cam1_contL3 == True) or (cam1_file_name == '')):
                                        cam1_file_name = datetime.now().strftime('%H%M%S')
                                        (self.save_dir1 / cam1_file_name).mkdir(parents=True, exist_ok=True)  # make dir
                                        cam1_contL3 = False
                                    
                                    file_crops = (self.save_dir1 / cam1_file_name / str(cam1_file_name+'_crop')).with_suffix('.jpg')     
                                    save_one_box(xyxy, imc, file = file_crops, BGR=True)
                                    if round(xywh[0],5 > 0.75):
                                        ready_snap = True

                                if ( ready_snap == True ):
                                        cam1_contL3 = False
                                        file_snap_adr0 = (self.save_dir1 / cam1_file_name / str(cam1_file_name+'_snap0')).with_suffix('.jpg')
                                        cv2.imwrite(file_snap_adr0, im0)

                                        # Link to pyQT
                                        self.cam1_ocr = ""
                                        self.cam1_snaps = file_snap_adr0
                                        #self.cam1_last_crops = dir_ocr
                                            
                                        # update logger
                                        LogThread_6.date_name = date_name
                                        LogThread_6.file1_ocr = self.cam1_ocr
                                        LogThread_6.file1_old = cam1_file_name
                                        
                                        # update variable
                                        tlMill = int(time.time() * 1000)
                                        cam1_contL1 = True

                

                        # Stream results
                        #im0 = annotator_plate.result()
                        self.cam1_pixmap_signal.emit(annotator_plate.result())
            
            except:Break
            else:
                if is_url:
                    ping = ping_server(a, int(b))
                    if ping == False:Break"""

    def __init__(self):
        super().__init__()
        # Load curent config json
        f_conf = open(root_p / 'config/current_conf.json')
        conf = json.load(f_conf)
        f_conf.close()
        self.dest = conf["STfd"]
        while True:
            self.log_date = datetime.now().strftime('%Y.%m.%d')
            self.current_time = datetime.now().strftime('%H:%M:%S')
            QThread.msleep(1)

            self.pathA = self.dest+'/Entrance'
            for list_pathA in os.listdir(self.pathA):
                for Inf_pathA in os.listdir(self.pathA +'/'+ list_pathA):
                    for Inp_pathA in os.listdir(self.pathA +'/'+ list_pathA+'/'+Inf_pathA):
                        Inp_re = Inp_pathA[::-1]
                        if 0 <= Inp_re.find("porc") < 5:
                            original = self.pathA +'/'+ list_pathA+'/'+Inf_pathA+'/'+Inp_pathA
                            target = self.dest+'/Crop'+'/Entrance'+'/'+list_pathA+'_'+Inf_pathA+'_'+Inp_pathA
                            self.source1.append(original)
                            shutil.copyfile(original, target) 
                            print(original)   
                            print(target)               
            
            self.pathB = self.dest+'/Exit'
            for list_pathB in os.listdir(self.pathB):
                for Inf_pathB in os.listdir(self.pathB +'/'+ list_pathB):
                    for Inp_pathB in os.listdir(self.pathB +'/'+ list_pathB+'/'+Inf_pathB):
                        Inp_re = Inp_pathB[::-1]
                        if 0 <= Inp_re.find("porc") < 5:
                            original = self.pathB +'/'+ list_pathB+'/'+Inf_pathB+'/'+Inp_pathB
                            target = self.dest+'/Crop'+'/Exit'+'/'+list_pathB+'_'+Inf_pathB+'_'+Inp_pathB
                            self.source2.append(original)
                            shutil.copyfile(original, target) 
                            print(original)   
                            print(target)   

            #print(self.source1)
            #print(self.source2)
            QThread.sleep(1)
            self.run_predic1()

if __name__ == "__main__":
    try:
        Main()
        
    except:
        sys.exit(0)
